{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Classification Using RNNs\n",
        "\n",
        "* Given the IMDB Movie Review Dataset, create an RNN model that predicts whether the given review is negative or positive.\n",
        "* You need to create your Dataset, Dataloader and Model. Keep your code modular and avoid hardcoding any parameter. This will allow you to experiment more easily.\n",
        "* Plot graphs for loss and accuracy for each epoch of a training loop. Try using wandb for logging training and validation losses, accuracies.\n",
        "* Use tqdm to keep track of the status of the training loop for an epoch"
      ],
      "metadata": {
        "id": "IdzeYa_AqAuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. RNN Model\n",
        "#### 1.1 Build a Dataset from the IMDB Movie Review Dataset by taking reviews with word count between 100 and 500. Perform text processing on the movie reviews and create a word to index mapping for representing any review as a list of numbers.\n",
        "#### 1.2 Create Dataloaders for the train, test and validation datasets with appropriate batch sizes.\n",
        "#### 1.3 Create the Model class for the RNN Model. Create functions for running model training and testing."
      ],
      "metadata": {
        "id": "Q6NN7DndxvFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets torchmetrics"
      ],
      "metadata": {
        "id": "DpR8af5l1FZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "DKFUUtZV09Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "\n",
        "# set seed for all possible random functions to ensure reproducibility\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic=True"
      ],
      "metadata": {
        "id": "DbVR4diJ4BxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "b-6qkjYjbhhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the IMDB review dataset. You can take the dataset from Huggingface\n",
        "imdb_dataset = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "O39R0KOF1Iw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the train set into train and validation in 80-20 split. Use the labels\n",
        "# to ensure that the ratio of the samples from each label is maintained"
      ],
      "metadata": {
        "id": "9a4eux6zDy7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text, tokenizer):\n",
        "  # Perform text preprocessing:\n",
        "  # 1. Removing numbers OR replace them with \"num\" token\n",
        "  # 2. Convert all characters to lowercase.\n",
        "  # 3. Tokenize the sentence into words\n",
        "  # You can use RegexpTokenizer from NLTK.\n",
        "\n",
        "  # You will experiment with stemming/lemmatization down the line\n",
        "  # so you can skip that for now\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "hJvJGlDo3rHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean(\"This IS 1 example sentence\", RegexpTokenizer(r'\\w+'))"
      ],
      "metadata": {
        "id": "aB5nrRbnaUbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a word to index dictionary so that each word in the training set\n",
        "# has a number associated with it. This allows to represent each sentence\n",
        "# as a series of numbers. Start the index with 1 instead of 0. The number\n",
        "# 0 will be used to denote padding, so that each sentence can have the\n",
        "# same length.\n",
        "# Keep track of the index since it will be used for representing new words\n",
        "# that were not part of the training vocabulary.\n",
        "# Also, make sure to not create dictionary on sentences with word count\n",
        "# not within the range\n",
        "\n",
        "def get_word2idx(corpus):\n",
        "  idx = 1\n",
        "  for sentence in tqdm(corpus, total=len(corpus), desc=\"Creating word2idx\"):\n",
        "    # process sentence\n",
        "    sentence = clean(sentence, tokenizer)\n",
        "\n",
        "    # drop sentences greater than maxlen or less than minlen\n",
        "\n",
        "    # for each word in sentence, check for entry in word2idx\n",
        "\n",
        "  return idx, word2idx"
      ],
      "metadata": {
        "id": "WEDLk2ZXUwVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a Dataset object to store each sentence as a tensor of numbers\n",
        "# along with the label. Make sure to add padding so that the tensor\n",
        "# for each sentence is of the same length. This will allow us to train\n",
        "# the model in batches.\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "  def __init__(self, dataset, split : str, minlen : int = 100, maxlen : int = 500):\n",
        "    self.count = 0 # total sentences you finally pick\n",
        "\n",
        "    # count total number of lines\n",
        "    len = len(dataset[split])\n",
        "\n",
        "    input_data = []\n",
        "    target_data = []\n",
        "\n",
        "    for idx, sentence in tqdm(enumerate(corpus), total=len, desc=f\"Transforming input text [{split}]\"):\n",
        "      # process sentence\n",
        "\n",
        "      # drop sentences greater than maxlen or less than minlen\n",
        "\n",
        "      # replace words with their index\n",
        "\n",
        "\n",
        "      self.count += 1\n",
        "\n",
        "    # pad the sentences upto maxlen\n",
        "    self.inputs = pad_sequence(input_data, batch_first = True)\n",
        "    self.targets = torch.tensor(target_data)\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return self.count\n",
        "\n",
        "  def __getitem__(self, index : int):\n",
        "    return self.inputs[index], self.targets[index]"
      ],
      "metadata": {
        "id": "M9g3b_S82ODM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the train dataset using the word2idx dictionary built using the train set\n",
        "train_ds = IMDBDataset(imdb_dataset, \"train\",minlen = 100, maxlen = 500)\n",
        "# create the validation and test dataset using the word2idx dictionary built using the train set\n",
        "\n"
      ],
      "metadata": {
        "id": "CGQ-5qXKG0Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_ds), len(val_ds), len(test_ds)"
      ],
      "metadata": {
        "id": "sgVEZqCNfIcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders using the dataset\n",
        "params = {\n",
        "    'batch_size':32,\n",
        "    'shuffle': True,\n",
        "    'num_workers': 2\n",
        "}\n",
        "\n",
        "train_dataloader = DataLoader(train_ds, **params)\n",
        "test_dataloader = DataLoader(val_ds, **params)\n",
        "test_dataloader = DataLoader(test_ds, **params)"
      ],
      "metadata": {
        "id": "FFQcp_oLTtDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model\n",
        "class RNNModel(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, embedding_dim, num_classes):\n",
        "    # call the init method of the parent\n",
        "\n",
        "    # define the layers\n",
        "\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    # run foward pass through the model\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "GAldcBovUotd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 256\n",
        "embedding_dim = 128\n",
        "learning_rate = 1e-3\n",
        "epochs = 5\n",
        "\n",
        "# create the model\n",
        "model = RNNModel(vocab_size, hidden_size, embedding_dim, num_classes).to(device)\n",
        "\n",
        "# create optimizer\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "rndJltB9ZDlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model training loop\n",
        "def train_model():\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    ## TRAINING STEP\n",
        "    model.train()\n",
        "    # train\n",
        "    for input_batch, output_batch in tqdm(trainloader, total = len(trainloader), desc = \"Training\"):\n",
        "\n",
        "    # Log metrics\n",
        "\n",
        "    ## VALIDATION STEP\n",
        "    model.eval()\n",
        "    # run validation\n",
        "    for input_batch, output_batch in tqdm(valloader, total = len(valloader), desc = \"Validation\"):\n",
        "\n",
        "    # Log metrics\n",
        "\n",
        "    # store best model\n",
        "\n",
        "  return train_losses, val_losses, val_accuracy"
      ],
      "metadata": {
        "id": "SDVn7UEtZSKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model testing loop\n"
      ],
      "metadata": {
        "id": "cSiAyfTir5zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "train_losses, val_losses, val_accuracy = train_model()"
      ],
      "metadata": {
        "id": "3WA5MGp3aCFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training and validation losses"
      ],
      "metadata": {
        "id": "s7MRQCJTooPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot validation accuracy"
      ],
      "metadata": {
        "id": "VLLkeoY59A5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the classification accuracy on test set\n"
      ],
      "metadata": {
        "id": "gUjgUQhtoyQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Incorporate stemming/lemmatization when doing text preprocessing using the NLTK library. What changes do you observe in accuracy ?"
      ],
      "metadata": {
        "id": "tHmA54UjvRh6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MMW-AMrYvRxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 In the Model class, experiment with only picking the last output and mean of all outputs in the RNN layer. What changes do you observe ?"
      ],
      "metadata": {
        "id": "23hqq3w8o2q3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-oUlRgmZAm-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Hyperparameter Tuning\n",
        "#### 2.1 Starting with the best configurations based on the above experiments, experiment with 5 different hyperparameter configurations. You can change the size of embedding layer, hidden state, batch in the dataloader.\n"
      ],
      "metadata": {
        "id": "dw2dyrnkrnxa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M0EuYk3Krodp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. After RNNs\n",
        "#### 3.1 Keeping all the parameters same, replace the RNN layer with the LSTM layer using nn.LSTM. What changes do you observe ? Explain why LSTM layer would affect performance."
      ],
      "metadata": {
        "id": "791zabclpSgi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lkbOdAMUrpgl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blv6XBI5GxO9"
      },
      "source": [
        "# SMAI Assignment - 2\n",
        "\n",
        "## Question 1: Naive Bayes and Clustering\n",
        "\n",
        "### Part 1: Naive Bayes\n",
        "\n",
        "[Files](https://drive.google.com/drive/folders/1OUVrOMp2jSSBDJSqvEyXDFTrhiyZnqit?usp=sharing)\n",
        "\n",
        "You will be performing Sentiment Analysis on a product review dataset with reviews from customers and star rating belonging to four classes (1,2,4,5). You can use sklearn for this question. Your tasks are as follows:\n",
        "\n",
        "1.   Clean the text by removing punctations and preprocess them using techniques such as stop word removal, stemming etc. You can explore anything!\n",
        "1.  Create BoW features using the word counts. You can choose the words that form the features such that the performance is optimised. Use the train-test split provided in `train_test_index.pickle` and report any interesting observations based on metrics such as accurarcy, precision, recall and f1 score (You can use Classification report in sklearn).\n",
        "1. Repeat Task 2 with TfIdf features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0nNsliRUjqd"
      },
      "outputs": [],
      "source": [
        "with open('train_test_index.pickle', 'rb') as handle:\n",
        "    train_test_index_dict = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86GXlinlUz8b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('product_reviews.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8GYzpkuUcs6"
      },
      "source": [
        "### Part 2: Clustering\n",
        "\n",
        "You will be performing kmeans clustering on the same product reviews dataset from Part 1. In this question, instead of statistically computing features, you will use the embeddings obtained from a neural sentiment analysis model (huggingface: siebert/sentiment-roberta-large-english).\n",
        "\n",
        "You can use sklearn for this question. Your tasks are as follows:\n",
        "\n",
        "\n",
        "1. Perform kmeans clustering using sklearn. Try various values for number of clusters (k) and plot the elbow curve. For each value of k, plot WCSS (Within-Cluster Sum of Square). WCSS is the sum of the squared distance between each point and the centroid in a cluster.\n",
        "1. Perform task 1 with cluster initialisation methods [k-means++, forgy (\"random\" in sklearn)].\n",
        "1. In this case, since the ground truth labels (star rating) are available we can evaluate the clustering using metrics like purity, nmi and rand score. Implement these metrics from scratch and evaluate the clustering. [Reference](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ddsz-nxGvfE"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "f = gzip.GzipFile('roberta_embeds.npy.gz', \"r\")\n",
        "embeds = np.load(f)\n",
        "print(embeds.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
